{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1e0d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DepthAI: 3.1.0 | OpenCV GUI available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[56213]: Class CVWindow is implemented in both /Users/manolisfragkidakis/miniconda3/envs/cameras/lib/python3.11/site-packages/cv2/cv2.abi3.so (0x17459a648) and /Users/manolisfragkidakis/miniconda3/envs/cameras/lib/python3.11/site-packages/palace.dylibs/libdepthai-core.dylib (0x31b281368). This may cause spurious casting failures and mysterious crashes. One of the duplicates must be removed or renamed.\n",
      "objc[56213]: Class CVView is implemented in both /Users/manolisfragkidakis/miniconda3/envs/cameras/lib/python3.11/site-packages/cv2/cv2.abi3.so (0x17459a670) and /Users/manolisfragkidakis/miniconda3/envs/cameras/lib/python3.11/site-packages/palace.dylibs/libdepthai-core.dylib (0x31b281390). This may cause spurious casting failures and mysterious crashes. One of the duplicates must be removed or renamed.\n",
      "objc[56213]: Class CVSlider is implemented in both /Users/manolisfragkidakis/miniconda3/envs/cameras/lib/python3.11/site-packages/cv2/cv2.abi3.so (0x17459a698) and /Users/manolisfragkidakis/miniconda3/envs/cameras/lib/python3.11/site-packages/palace.dylibs/libdepthai-core.dylib (0x31b2813b8). This may cause spurious casting failures and mysterious crashes. One of the duplicates must be removed or renamed.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Imports, config, GUI probe ---\n",
    "\n",
    "import sys, time, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import depthai as dai\n",
    "\n",
    "# Preview / record settings\n",
    "PREVIEW_W, PREVIEW_H, FPS = 960, 540, 30\n",
    "WINDOW_NAME = \"OAK-D Pose (cam0 | cam1)\"\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# Fast/low-res mode: try to pull a small preview from the device (highest FPS),\n",
    "# otherwise fall back to full-res and downscale on host.\n",
    "FAST_LOWRES = True          # toggle this to False to go back to full-res\n",
    "LOWRES_W, LOWRES_H = 320, 180   # tiny, very fast for MediaPipe\n",
    "TARGET_FPS = 60                 # request high fps if the node allows it\n",
    "\n",
    "# If fast mode is on, also shrink preview/record sizes to match for max throughput\n",
    "if FAST_LOWRES:\n",
    "    PREVIEW_W, PREVIEW_H = LOWRES_W, LOWRES_H\n",
    "\n",
    "\n",
    "def can_open_cv_window(force_headless: bool = False) -> bool:\n",
    "    if force_headless:\n",
    "        return False\n",
    "    try:\n",
    "        cv2.namedWindow(\"_probe_\", cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"_probe_\", np.zeros((1, 1, 3), dtype=\"uint8\"))\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyWindow(\"_probe_\")\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "HAS_GUI = can_open_cv_window(False)\n",
    "print(f\"DepthAI: {dai.__version__} | OpenCV GUI available: {HAS_GUI}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259705e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Device discovery helpers (tolerant) ---\n",
    "\n",
    "def list_devices():\n",
    "    return dai.Device.getAllAvailableDevices()\n",
    "\n",
    "def pick_devices(required=2, allow_less=True):\n",
    "    devs = list_devices()\n",
    "    if len(devs) >= required:\n",
    "        return devs[:required]\n",
    "    if allow_less and len(devs) > 0:\n",
    "        print(f\"[WARN] Expected {required} devices, found {len(devs)}. Running with {len(devs)}.\")\n",
    "        return devs\n",
    "    raise RuntimeError(f\"Need {required} OAK devices, found {len(devs)}.\")\n",
    "\n",
    "def pick_rgb_socket(device: dai.Device):\n",
    "    sockets = list(device.getConnectedCameras())\n",
    "    if not sockets:\n",
    "        raise RuntimeError(\"No cameras connected/reported on this device.\")\n",
    "    for s in sockets:\n",
    "        if str(s).endswith(\"RGB\") or s == getattr(dai.CameraBoardSocket, \"RGB\", None):\n",
    "            return s\n",
    "    return sockets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a09c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: MediaPipe Pose wrapper ---\n",
    "\n",
    "class PoseEstimator:\n",
    "    def __init__(self, model_complexity=1):\n",
    "        mp_pose = mp.solutions.pose\n",
    "        self.pose = mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=model_complexity,\n",
    "            enable_segmentation=False,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5,\n",
    "        )\n",
    "        self.drawer = mp.solutions.drawing_utils\n",
    "        self.styles = mp.solutions.drawing_styles\n",
    "        self.connections = mp_pose.POSE_CONNECTIONS\n",
    "\n",
    "    def process_and_draw(self, frame_bgr):\n",
    "        # frame_bgr is modified in-place (drawing)\n",
    "        res = self.pose.process(cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB))\n",
    "        if res.pose_landmarks:\n",
    "            self.drawer.draw_landmarks(\n",
    "                frame_bgr, res.pose_landmarks, self.connections,\n",
    "                landmark_drawing_spec=self.styles.get_default_pose_landmarks_style()\n",
    "            )\n",
    "        return frame_bgr\n",
    "\n",
    "    def close(self):\n",
    "        self.pose.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8bd2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Recording & playback utilities ---\n",
    "\n",
    "class DualRecorder:\n",
    "    def __init__(self, out_dir, fps=FPS, size=(PREVIEW_W, PREVIEW_H)):\n",
    "        out_dir = Path(out_dir)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.f0 = str(out_dir / f\"cam0_{ts}.mp4\")\n",
    "        self.f1 = str(out_dir / f\"cam1_{ts}.mp4\")\n",
    "        self.w0 = cv2.VideoWriter(self.f0, fourcc, fps, size)\n",
    "        self.w1 = cv2.VideoWriter(self.f1, fourcc, fps, size)\n",
    "        self.active = True\n",
    "        print(f\"[REC] → {self.f0}\\n[REC] → {self.f1}\")\n",
    "\n",
    "    def write(self, f0, f1):\n",
    "        if not self.active:\n",
    "            return\n",
    "        if f0 is not None: self.w0.write(f0)\n",
    "        if f1 is not None: self.w1.write(f1)\n",
    "\n",
    "    def stop(self):\n",
    "        if self.active:\n",
    "            self.w0.release(); self.w1.release()\n",
    "            self.active = False\n",
    "            print(\"[REC] stopped\")\n",
    "\n",
    "class DualPlayer:\n",
    "    def __init__(self, f0, f1):\n",
    "        self.cap0, self.cap1 = cv2.VideoCapture(f0), cv2.VideoCapture(f1)\n",
    "        if not self.cap0.isOpened() or not self.cap1.isOpened():\n",
    "            raise RuntimeError(\"Playback files not found or unreadable.\")\n",
    "        print(f\"[PLAY] {f0}\\n[PLAY] {f1}\")\n",
    "\n",
    "    def read(self):\n",
    "        ok0, f0 = self.cap0.read()\n",
    "        ok1, f1 = self.cap1.read()\n",
    "        if not ok0 and not ok1:\n",
    "            return None, None, False\n",
    "        return (f0 if ok0 else None), (f1 if ok1 else None), True\n",
    "\n",
    "    def stop(self):\n",
    "        self.cap0.release(); self.cap1.release()\n",
    "\n",
    "def latest_pair(base=\"recordings\"):\n",
    "    base = Path(base)\n",
    "    if not base.exists(): return None, None\n",
    "    dirs = sorted([p for p in base.iterdir() if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for d in dirs:\n",
    "        c0 = max(d.glob(\"cam0_*.mp4\"), default=None, key=lambda p: p.stat().st_mtime if p.exists() else 0)\n",
    "        c1 = max(d.glob(\"cam1_*.mp4\"), default=None, key=lambda p: p.stat().st_mtime if p.exists() else 0)\n",
    "        if c0 and c1: return str(c0), str(c1)\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50acf1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Build per-device pipelines using the new API style (fast/low-res aware) ---\n",
    "\n",
    "def _request_preview_or_full(builder, width, height):\n",
    "    \"\"\"\n",
    "    Try to get a low-res preview output from the on-device scaler.\n",
    "    Fallback to full resolution if preview isn't available in this build.\n",
    "    Returns (queue, using_preview: bool)\n",
    "    \"\"\"\n",
    "    # Prefer preview (low-res)\n",
    "    if FAST_LOWRES and hasattr(builder, \"requestPreviewOutput\"):\n",
    "        try:\n",
    "            # Some builds take (w,h), some use named args — try both\n",
    "            try:\n",
    "                qbuilder = builder.requestPreviewOutput(width, height)\n",
    "            except TypeError:\n",
    "                qbuilder = builder.requestPreviewOutput(width=width, height=height)\n",
    "            q = qbuilder.createOutputQueue()\n",
    "            return q, True\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback: full resolution\n",
    "    q = builder.requestFullResolutionOutput().createOutputQueue()\n",
    "    return q, False\n",
    "\n",
    "\n",
    "def make_output_queue_for_device(device: dai.Device):\n",
    "    \"\"\"\n",
    "    Using the 'new' API:\n",
    "      with dai.Pipeline(device) as pipeline:\n",
    "          cam = pipeline.create(dai.node.Camera).build(socket)\n",
    "          (preview or full) -> OutputQueue\n",
    "    Returns (pipeline_context_manager, socket_name).\n",
    "    \"\"\"\n",
    "    socket = pick_rgb_socket(device)\n",
    "    socket_name = str(socket)\n",
    "\n",
    "    class _PipeWrapper:\n",
    "        def __enter__(self):\n",
    "            self.cm = dai.Pipeline(device)\n",
    "            self.pipeline = self.cm.__enter__()\n",
    "\n",
    "            # Build camera node for this socket\n",
    "            cam_builder = self.pipeline.create(dai.node.Camera).build(socket)\n",
    "\n",
    "            # If the builder exposes FPS control in this API, request high FPS\n",
    "            for attr in (\"setFps\", \"setFrameRate\"):\n",
    "                if hasattr(cam_builder, attr):\n",
    "                    try:\n",
    "                        getattr(cam_builder, attr)(TARGET_FPS)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # Prefer preview output (low-res) if available\n",
    "            self.queue, self.using_preview = _request_preview_or_full(\n",
    "                cam_builder, LOWRES_W, LOWRES_H\n",
    "            )\n",
    "\n",
    "            self.pipeline.start()\n",
    "            return self\n",
    "\n",
    "        def __exit__(self, exc_type, exc, tb):\n",
    "            try:\n",
    "                if hasattr(self.pipeline, \"stop\"):\n",
    "                    self.pipeline.stop()\n",
    "            except Exception:\n",
    "                pass\n",
    "            return self.cm.__exit__(exc_type, exc, tb)\n",
    "\n",
    "        def isRunning(self):\n",
    "            return self.pipeline.isRunning()\n",
    "\n",
    "        def get(self):\n",
    "            return self.queue.get()\n",
    "\n",
    "    return _PipeWrapper(), socket_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8147e429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected devices: 2\n",
      "[0] mxid=<unknown> | attrs=['deviceId', 'getDeviceId', 'getXLinkDeviceDesc', 'name', 'platform', 'protocol', 'state', 'status']\n",
      "[1] mxid=<unknown> | attrs=['deviceId', 'getDeviceId', 'getXLinkDeviceDesc', 'name', 'platform', 'protocol', 'state', 'status']\n"
     ]
    }
   ],
   "source": [
    "# --- Device probe ---\n",
    "import depthai as dai\n",
    "\n",
    "devs = dai.Device.getAllAvailableDevices()\n",
    "print(f\"Detected devices: {len(devs)}\")\n",
    "for i, d in enumerate(devs):\n",
    "    mx = getattr(d, \"mxid\", None)\n",
    "    if mx is None and hasattr(d, \"getMxId\"):\n",
    "        try: mx = d.getMxId()\n",
    "        except: mx = None\n",
    "    print(f\"[{i}] mxid={mx or '<unknown>'} | attrs={sorted(set([k for k in dir(d) if not k.startswith('_')]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07d5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Main loop (works with 1 or 2 devices) ---\n",
    "\n",
    "def main(out_dir=\"recordings\", auto_record=False):\n",
    "    dev_infos = pick_devices(required=2, allow_less=True)\n",
    "\n",
    "    # Open devices we actually have\n",
    "    devices = [dai.Device(di) for di in dev_infos]\n",
    "\n",
    "    # Build per-device pipeline/queue wrappers\n",
    "    pipes = []\n",
    "    names = []\n",
    "    for dev in devices:\n",
    "        p, name = make_output_queue_for_device(dev)\n",
    "        pipes.append(p)\n",
    "        names.append(name)\n",
    "\n",
    "    # Pose estimators (one per stream we have)\n",
    "    poses = [PoseEstimator() for _ in pipes]\n",
    "\n",
    "    # Session dir\n",
    "    session_dir = Path(out_dir) / datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    recorder = None\n",
    "    player = None\n",
    "    live = True\n",
    "\n",
    "    if HAS_GUI:\n",
    "        cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(WINDOW_NAME, PREVIEW_W * 2, PREVIEW_H)\n",
    "\n",
    "    if auto_record:\n",
    "        recorder = DualRecorder(session_dir, fps=FPS, size=(PREVIEW_W, PREVIEW_H))\n",
    "\n",
    "    t_prev = time.time()\n",
    "\n",
    "    try:\n",
    "        # Enter all available pipelines (1 or 2)\n",
    "        ctxs = [p.__enter__() for p in pipes]\n",
    "        try:\n",
    "            # Helper to check running state\n",
    "            def all_running():\n",
    "                return all(p.isRunning() for p in ctxs)\n",
    "\n",
    "            while all_running():\n",
    "                if live:\n",
    "                    # Read a frame per available pipe\n",
    "                    frames = [ctx.get().getCvFrame() for ctx in ctxs]\n",
    "                else:\n",
    "                    f0, f1, ok = player.read()\n",
    "                    if not ok:\n",
    "                        print(\"[PLAY] done.\")\n",
    "                        if player: player.stop(); player = None\n",
    "                        live = True\n",
    "                        continue\n",
    "                    frames = [f0, f1]\n",
    "\n",
    "                # Process frames we have\n",
    "                processed = []\n",
    "                for f, pose in zip(frames, poses):\n",
    "                    if f is None:\n",
    "                        processed.append(None)\n",
    "                        continue\n",
    "                    if (f.shape[1], f.shape[0]) != (PREVIEW_W, PREVIEW_H):\n",
    "                        f = cv2.resize(f, (PREVIEW_W, PREVIEW_H))\n",
    "                    processed.append(pose.process_and_draw(f))\n",
    "\n",
    "                # If only one device, synthesize the second pane as black\n",
    "                if len(processed) == 1:\n",
    "                    if processed[0] is None:\n",
    "                        left = np.zeros((PREVIEW_H, PREVIEW_W, 3), np.uint8)\n",
    "                    else:\n",
    "                        left = processed[0]\n",
    "                    right = np.zeros_like(left)\n",
    "                else:\n",
    "                    left = processed[0] if processed[0] is not None else np.zeros((PREVIEW_H, PREVIEW_W, 3), np.uint8)\n",
    "                    right = processed[1] if processed[1] is not None else np.zeros((PREVIEW_H, PREVIEW_W, 3), np.uint8)\n",
    "\n",
    "                combined = cv2.hconcat([left, right])\n",
    "\n",
    "                # FPS & status\n",
    "                now = time.time()\n",
    "                fps = 1.0 / max(1e-6, now - t_prev)\n",
    "                t_prev = now\n",
    "                label = (\"LIVE\" if live else \"PLAY\") + (\" + REC\" if (recorder and recorder.active) else \"\")\n",
    "                cv2.putText(combined, f\"{label}  FPS: {fps:.1f}\", (10, 30), FONT, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "                # Show\n",
    "                key = 255\n",
    "                if HAS_GUI:\n",
    "                    cv2.imshow(WINDOW_NAME, combined)\n",
    "                    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "                # Record (always write two panes for consistency)\n",
    "                if recorder and recorder.active:\n",
    "                    # If we only have one real camera, the second file will be black video.\n",
    "                    recorder.write(left, right)\n",
    "\n",
    "                # Hotkeys\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('r') and HAS_GUI:\n",
    "                    if recorder and recorder.active:\n",
    "                        recorder.stop(); recorder = None\n",
    "                    else:\n",
    "                        recorder = DualRecorder(session_dir, fps=FPS, size=(PREVIEW_W, PREVIEW_H))\n",
    "                elif key == ord('p') and HAS_GUI:\n",
    "                    if live:\n",
    "                        f0path, f1path = latest_pair(out_dir)\n",
    "                        if not f0path or not f1path:\n",
    "                            print(\"[PLAY] No recordings found.\")\n",
    "                        else:\n",
    "                            player = DualPlayer(f0path, f1path)\n",
    "                            live = False\n",
    "                    else:\n",
    "                        if player: player.stop(); player = None\n",
    "                        live = True\n",
    "                elif key == ord('s') and HAS_GUI:\n",
    "                    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    cv2.imwrite(str(session_dir / f\"cam0_snap_{ts}.png\"), left)\n",
    "                    cv2.imwrite(str(session_dir / f\"cam1_snap_{ts}.png\"), right)\n",
    "                    print(\"[SNAP] saved\")\n",
    "\n",
    "        finally:\n",
    "            # Exit all pipeline contexts\n",
    "            for p in pipes[::-1]:\n",
    "                try: p.__exit__(None, None, None)\n",
    "                except: pass\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if recorder and recorder.active:\n",
    "            recorder.stop()\n",
    "        if player:\n",
    "            player.stop()\n",
    "        for dev in devices:\n",
    "            try: dev.close()\n",
    "            except: pass\n",
    "        for pose in poses:\n",
    "            try: pose.close()\n",
    "            except: pass\n",
    "        if HAS_GUI:\n",
    "            try: cv2.destroyAllWindows()\n",
    "            except: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aefa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/ty4phdkj5814s93r8dpd5vqm0000gn/T/ipykernel_56213/2833358920.py:20: DeprecationWarning: RGB is deprecated, use CAM_A or address camera by name instead.\n",
      "  if str(s).endswith(\"RGB\") or s == getattr(dai.CameraBoardSocket, \"RGB\", None):\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763039863.702212 5936360 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1763039863.709928 5936360 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1763039863.781230 5936874 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763039863.787733 5936887 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763039863.794314 5936874 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763039863.800789 5936887 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1763039864.482181 5936879 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-13 14:18:04.644] [depthai] [error] Communication exception - possible device error/misconfiguration. Original message 'Couldn't read data from stream: '__x_0_0' (X_LINK_ERROR)'\n",
      "[14442C10B11ED7D600] [1.1.3] [1763039885.750] [host] [warning] Closed connection\n",
      "\n",
      "[14442C10B11ED7D600] [1.1.3] [1763039885.750] [host] [warning] Attempting to reconnect. Timeout is 10000ms\n",
      "\n",
      "[14442C10B11ED7D600] [1.1.3] [1763039890.975] [host] [warning] Reconnection successful\n",
      "\n",
      "[2025-11-13 14:18:20.518] [depthai] [error] Communication exception - possible device error/misconfiguration. Original message 'Couldn't read data from stream: '__x_0_0' (X_LINK_ERROR)'\n",
      "[14442C10B11ED7D600] [1.1.3] [1763039901.500] [host] [warning] Closed connection\n",
      "\n",
      "[14442C10B11ED7D600] [1.1.3] [1763039901.500] [host] [warning] Attempting to reconnect. Timeout is 10000ms\n",
      "\n",
      "[14442C10B11ED7D600] [1.1.3] [1763039908.384] [host] [warning] Reconnection successful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main(out_dir=\"recordings\", auto_record=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7b54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
